{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d33630-cfde-4c41-a39b-088e2fd88b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9090b6c-1c64-4dae-9b8b-a42e9c08a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "new_path = '/root/autodl-tmp/project'\n",
    "os.chdir(new_path)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3dd1604-8852-4ca1-b2f3-30420cb5b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def load_config(config_path='config.json'):\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    args = SimpleNamespace(**config)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef1d71f-aeed-4e60-8eda-c7df83c18978",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_config() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20721144-0c52-4b21-a47c-74b2c581ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtDataset(Dataset):\n",
    "    def __init__(self, manifest_path, root_dir, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        with open(manifest_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                img_rel, label_rel = line.strip().rsplit(',', 1)\n",
    "                img_path = os.path.join(root_dir, img_rel.replace('\\\\', '/'))\n",
    "                #img_path = os.path.join(root_dir, img_rel.strip())\n",
    "                #label_path = os.path.join(root_dir, label_rel.strip())\n",
    "                label_path = os.path.join(self.root_dir, label_rel.replace('\\\\', '/'))\n",
    "                if not os.path.exists(img_path) or not os.path.exists(label_path):\n",
    "                    continue\n",
    "                with open(label_path, 'r', encoding='utf-8') as lf:\n",
    "                    label = int(lf.read().strip())\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((args.im_size, args.im_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cefe84f-62f3-4975-9bc1-b4a9f7cc4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('genimgs/Genre128GANAE/gen64', exist_ok=True)\n",
    "os.makedirs('genimgs/Genre128GANAE/real', exist_ok=True)\n",
    "os.makedirs('genimgs/Genre128GANAE/gen128', exist_ok=True)\n",
    "os.makedirs('models/Genre128GANAE', exist_ok=True)\n",
    "os.makedirs('figs/Genre128GANAE', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbf91c8-507c-4c74-909f-52b189b20a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(args.zdim + args.n_classes, 512 * 4 * 4),\n",
    "            nn.BatchNorm1d(512 * 4 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        def block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            block(512, 512),   # 8×8\n",
    "            block(512, 256),   # 16×16\n",
    "            block(256, 128),   # 32×32\n",
    "            block(128, 64),    # 64×64\n",
    "            block(64, 32),     # 128×128\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        y = F.one_hot(labels, args.n_classes).float()\n",
    "        x = torch.cat([z, y], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        out = self.deconv(x)\n",
    "        return out  # 128×128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c7086c-41f9-46c8-9364-b06d60a0369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def down(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                spectral_norm(nn.Conv2d(in_c, out_c, 4, 2, 1)),  # >>> MOD <<< SpectralNorm\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            down(3, 64),   # 64\n",
    "            down(64, 128), # 32\n",
    "            down(128, 256),# 16\n",
    "            down(256, 512),# 8\n",
    "            down(512, 512) # 4\n",
    "        )\n",
    "        self.adv_head = spectral_norm(nn.Conv2d(512, 1, 4))  # 4→1 logit\n",
    "        self.cls_head = nn.Linear(512*4*4, args.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.main(x)\n",
    "        adv_out = self.adv_head(feat).view(x.size(0))  # [B]\n",
    "        cls_logits = self.cls_head(feat.view(x.size(0), -1))\n",
    "        return adv_out, cls_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1922e521-0fa2-46cd-a662-5b69e5d80370",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train(args):\n",
    "    losses_D, losses_G, iterations = [], [], []\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "    \n",
    "    opt_G = optim.Adam(G.parameters(), lr=args.lr_init, betas=(0.0, 0.9))   # >>> MOD <<<\n",
    "    opt_D = optim.Adam(D.parameters(), lr=args.lr_init, betas=(0.0, 0.9))\n",
    "    \n",
    "    train_set = ArtDataset(os.path.join(args.data_root, 'genre-train-index.csv'), args.data_root, transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    \n",
    "    # -----------------------\n",
    "    #  6. 损失函数\n",
    "    # -----------------------\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # -----------------------\n",
    "    #  7. 训练循环\n",
    "    # -----------------------\n",
    "    iter_count = 0\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for real_imgs, real_labels in train_loader:\n",
    "            iter_count += 1\n",
    "            real_imgs, real_labels = real_imgs.to(device), real_labels.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "    \n",
    "            #  ===== 1.  训练 Discriminator =====\n",
    "            D.zero_grad()\n",
    "            # real\n",
    "            real_adv, real_cls = D(real_imgs)\n",
    "            loss_D_real = F.relu(1.0 - real_adv).mean()\n",
    "            loss_cls_real = ce_loss(real_cls, real_labels)\n",
    "    \n",
    "            # fake\n",
    "            z = torch.randn(batch_size, args.zdim, device=device)\n",
    "            rand_labels = torch.randint(0, args.n_classes, (batch_size,), device=device)  # >>> MOD <<<\n",
    "            fake_imgs = G(z, rand_labels).detach()\n",
    "            fake_adv, _ = D(fake_imgs)\n",
    "            loss_D_fake = F.relu(1.0 + fake_adv).mean()\n",
    "    \n",
    "            loss_D = loss_D_real + loss_D_fake + 0.1 * loss_cls_real  # 权重平衡\n",
    "            loss_D.backward()\n",
    "            opt_D.step()\n",
    "    \n",
    "            #  ===== 2.  训练 Generator =====\n",
    "            G.zero_grad()\n",
    "            z = torch.randn(batch_size, args.zdim, device=device)\n",
    "            rand_labels = torch.randint(0, args.n_classes, (batch_size,), device=device)\n",
    "            gen_imgs = G(z, rand_labels)\n",
    "            adv_out, cls_out = D(gen_imgs)\n",
    "            loss_G_adv = -adv_out.mean()\n",
    "            loss_G_cls = ce_loss(cls_out, rand_labels)\n",
    "            loss_G = loss_G_adv + 0.1 * loss_G_cls  # 权重平衡\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "    \n",
    "            #  ===== 3.  Logging / save =====\n",
    "            if iter_count % args.display_iter == 0:\n",
    "                losses_D.append(loss_D.item())\n",
    "                losses_G.append(loss_G.item())\n",
    "                iterations.append(iter_count)\n",
    "                print(f\"Iter {iter_count} | D_loss: {loss_D.item():.4f} | G_loss: {loss_G.item():.4f}\")\n",
    "            if iter_count % args.save_iter == 0:\n",
    "                torch.save(G.state_dict(), f'models/Genre128GANAE_mod/G_{iter_count}.pth')\n",
    "                torch.save(D.state_dict(), f'models/Genre128GANAE_mod/D_{iter_count}.pth')\n",
    "                with torch.no_grad():\n",
    "                    vis_img = (gen_imgs[:64] + 1) / 2  # 反归一化\n",
    "                    save_image(vis_img, f'genimgs/Genre128GANAE_mod/gen/{iter_count}.png', nrow=8)\n",
    "                    vis_real = (real_imgs[:64] + 1) / 2\n",
    "                    save_image(vis_real, f'genimgs/Genre128GANAE_mod/real/{iter_count}.png', nrow=8)\n",
    "                    \n",
    "    \n",
    "    print(\"Training finished!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4a4e08-a7e5-4851-bcdf-8918c326882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    losses_D, losses_G, iterations = [], [], []\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    G = Generator().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=args.lr_init, betas=(0.0, 0.9))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=args.lr_init, betas=(0.0, 0.9))\n",
    "\n",
    "    train_set = ArtDataset(\n",
    "        os.path.join(args.data_root, 'genre-train-index.csv'),\n",
    "        args.data_root, transform\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=args.batch_size,\n",
    "        shuffle=True, num_workers=4, drop_last=True\n",
    "    )\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if args.resume_G and args.resume_D:\n",
    "        G.load_state_dict(torch.load(args.resume_G, map_location=device))\n",
    "        D.load_state_dict(torch.load(args.resume_D, map_location=device))\n",
    "        iter_count = args.init_iter\n",
    "        print(f\"Resumed from iter {iter_count}: loaded {args.resume_G} and {args.resume_D}\")\n",
    "    else:\n",
    "        iter_count = 0\n",
    "        \n",
    "    for epoch in range(args.num_epochs):\n",
    "        for real_imgs, real_labels in train_loader:\n",
    "            iter_count += 1\n",
    "            real_imgs, real_labels = real_imgs.to(device), real_labels.to(device)\n",
    "            bsz = real_imgs.size(0)\n",
    "\n",
    "            # ===== 1. 训练 Discriminator =====\n",
    "            D.zero_grad()\n",
    "            # real\n",
    "            real_adv, real_cls = D(real_imgs)\n",
    "            loss_D_real = F.relu(1.0 - real_adv).mean()\n",
    "            loss_cls_real = ce_loss(real_cls, real_labels)\n",
    "\n",
    "            # fake\n",
    "            z = torch.randn(bsz, args.zdim, device=device)\n",
    "            rand_labels = torch.randint(0, args.n_classes, (bsz,), device=device)\n",
    "            fake_imgs = G(z, rand_labels).detach()\n",
    "            fake_adv, _    = D(fake_imgs)\n",
    "            loss_D_fake   = F.relu(1.0 + fake_adv).mean()\n",
    "\n",
    "            loss_D = loss_D_real + loss_D_fake + 0.1 * loss_cls_real\n",
    "            loss_D.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # ===== 2. 训练 Generator =====\n",
    "            G.zero_grad()\n",
    "            z = torch.randn(bsz, args.zdim, device=device)\n",
    "            rand_labels = torch.randint(0, args.n_classes, (bsz,), device=device)\n",
    "            gen_imgs = G(z, rand_labels)\n",
    "            adv_out, cls_out = D(gen_imgs)\n",
    "            loss_G_adv = -adv_out.mean()\n",
    "            loss_G_cls = ce_loss(cls_out, rand_labels)\n",
    "            loss_G = loss_G_adv + 0.1 * loss_G_cls\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            # ===== 3. Logging & Save =====\n",
    "            if iter_count % args.display_iter == 0:\n",
    "                losses_D.append(loss_D.item())\n",
    "                losses_G.append(loss_G.item())\n",
    "                iterations.append(iter_count)\n",
    "                print(f\"Iter {iter_count} | D_loss: {loss_D.item():.4f} | G_loss: {loss_G.item():.4f}\")\n",
    "\n",
    "            if iter_count % args.save_iter == 0:\n",
    "                # 保存模型\n",
    "                torch.save(G.state_dict(), f'models/Genre128GANAE/G_{iter_count}.pth')\n",
    "                torch.save(D.state_dict(), f'models/Genre128GANAE/D_{iter_count}.pth')\n",
    "                with torch.no_grad():\n",
    "                    # 128×128 生成图\n",
    "                    vis128 = (gen_imgs[:64] + 1) / 2\n",
    "                    save_image(vis128,\n",
    "                               f'genimgs/Genre128GANAE/gen128/{iter_count}.png',\n",
    "                               nrow=8)\n",
    "                    # 64×64 下采样生成图\n",
    "                    vis64 = (F.avg_pool2d(gen_imgs, 2)[:64] + 1) / 2\n",
    "                    save_image(vis64,\n",
    "                               f'genimgs/Genre128GANAE/gen64/{iter_count}.png',\n",
    "                               nrow=8)\n",
    "                    # 真实图\n",
    "                    vis_real = (real_imgs[:64] + 1) / 2\n",
    "                    save_image(vis_real,\n",
    "                               f'genimgs/Genre128GANAE/real/{iter_count}.png',\n",
    "                               nrow=8)\n",
    "\n",
    "    # ===== 4. 画并保存 loss 曲线 =====\n",
    "    plt.figure()\n",
    "    plt.plot(iterations, losses_D, label='D_loss')\n",
    "    plt.plot(iterations, losses_G, label='G_loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figs/Genre128GANAE/loss_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a11ec-312c-4cce-8108-45806c85da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from iter 70000: loaded models/Genre128GANAE/G_70000.pth and models/Genre128GANAE/D_70000.pth\n",
      "Iter 70100 | D_loss: 1.8538 | G_loss: 0.6718\n",
      "Iter 70200 | D_loss: 1.7938 | G_loss: 0.4876\n",
      "Iter 70300 | D_loss: 1.7765 | G_loss: 0.1833\n",
      "Iter 70400 | D_loss: 1.7578 | G_loss: 0.3190\n",
      "Iter 70500 | D_loss: 2.1508 | G_loss: 1.4386\n",
      "Iter 70600 | D_loss: 1.6656 | G_loss: 0.4800\n",
      "Iter 70700 | D_loss: 1.7859 | G_loss: 0.5015\n",
      "Iter 70800 | D_loss: 1.6681 | G_loss: 0.3286\n",
      "Iter 70900 | D_loss: 1.8388 | G_loss: 0.6184\n",
      "Iter 71000 | D_loss: 2.0600 | G_loss: -0.5961\n",
      "Iter 71100 | D_loss: 1.7951 | G_loss: 0.4537\n",
      "Iter 71200 | D_loss: 1.9431 | G_loss: 1.4342\n",
      "Iter 71300 | D_loss: 1.7684 | G_loss: 0.1655\n",
      "Iter 71400 | D_loss: 1.8232 | G_loss: 0.1735\n",
      "Iter 71500 | D_loss: 1.7191 | G_loss: 0.4928\n",
      "Iter 71600 | D_loss: 1.7808 | G_loss: 0.6730\n",
      "Iter 71700 | D_loss: 1.7620 | G_loss: 0.3916\n",
      "Iter 71800 | D_loss: 1.6733 | G_loss: 0.5845\n",
      "Iter 71900 | D_loss: 1.7601 | G_loss: 0.6060\n",
      "Iter 72000 | D_loss: 1.8017 | G_loss: 0.2939\n",
      "Iter 72100 | D_loss: 1.7473 | G_loss: 0.2502\n",
      "Iter 72200 | D_loss: 1.7212 | G_loss: 0.2326\n",
      "Iter 72300 | D_loss: 1.6614 | G_loss: 1.0999\n",
      "Iter 72400 | D_loss: 1.7530 | G_loss: 0.1533\n",
      "Iter 72500 | D_loss: 1.6708 | G_loss: 0.4479\n",
      "Iter 72600 | D_loss: 1.6794 | G_loss: 0.4607\n",
      "Iter 72700 | D_loss: 1.6834 | G_loss: 0.3371\n",
      "Iter 72800 | D_loss: 1.9075 | G_loss: 0.6422\n",
      "Iter 72900 | D_loss: 1.7317 | G_loss: 0.6314\n",
      "Iter 73000 | D_loss: 1.7263 | G_loss: 0.8867\n",
      "Iter 73100 | D_loss: 2.2099 | G_loss: 1.0452\n",
      "Iter 73200 | D_loss: 1.6568 | G_loss: 0.5915\n",
      "Iter 73300 | D_loss: 1.6511 | G_loss: 0.6129\n",
      "Iter 73400 | D_loss: 1.7528 | G_loss: 1.1968\n",
      "Iter 73500 | D_loss: 1.7087 | G_loss: 0.6544\n",
      "Iter 73600 | D_loss: 1.7351 | G_loss: -0.1456\n",
      "Iter 73700 | D_loss: 1.8195 | G_loss: -0.2150\n",
      "Iter 73800 | D_loss: 1.7756 | G_loss: 0.6083\n",
      "Iter 73900 | D_loss: 1.7284 | G_loss: 0.3981\n",
      "Iter 74000 | D_loss: 1.6695 | G_loss: 0.3267\n",
      "Iter 74100 | D_loss: 1.7715 | G_loss: 0.2388\n",
      "Iter 74200 | D_loss: 1.6383 | G_loss: 0.3103\n",
      "Iter 74300 | D_loss: 1.7285 | G_loss: 0.2663\n",
      "Iter 74400 | D_loss: 1.7353 | G_loss: 0.5743\n",
      "Iter 74500 | D_loss: 1.7098 | G_loss: 0.4274\n",
      "Iter 74600 | D_loss: 1.7629 | G_loss: 0.1691\n",
      "Iter 74700 | D_loss: 1.7850 | G_loss: 0.5067\n",
      "Iter 74800 | D_loss: 1.6943 | G_loss: 0.2432\n",
      "Iter 74900 | D_loss: 1.6807 | G_loss: 0.5117\n",
      "Iter 75000 | D_loss: 1.5980 | G_loss: 0.4558\n",
      "Iter 75100 | D_loss: 1.6893 | G_loss: 0.3773\n",
      "Iter 75200 | D_loss: 1.6811 | G_loss: 0.4983\n",
      "Iter 75300 | D_loss: 1.7827 | G_loss: 0.2746\n",
      "Iter 75400 | D_loss: 1.8447 | G_loss: 0.6330\n",
      "Iter 75500 | D_loss: 1.6862 | G_loss: 0.3417\n",
      "Iter 75600 | D_loss: 1.8086 | G_loss: 0.1709\n",
      "Iter 75700 | D_loss: 1.6842 | G_loss: 0.8872\n",
      "Iter 75800 | D_loss: 1.7445 | G_loss: 0.1981\n",
      "Iter 75900 | D_loss: 1.7787 | G_loss: 0.4227\n",
      "Iter 76000 | D_loss: 1.7403 | G_loss: 0.1001\n",
      "Iter 76100 | D_loss: 1.6759 | G_loss: 0.3646\n",
      "Iter 76200 | D_loss: 2.1550 | G_loss: 1.1340\n",
      "Iter 76300 | D_loss: 1.6392 | G_loss: 0.1945\n",
      "Iter 76400 | D_loss: 1.7885 | G_loss: 0.6078\n",
      "Iter 76500 | D_loss: 1.6845 | G_loss: 0.4169\n",
      "Iter 76600 | D_loss: 1.7117 | G_loss: 0.4746\n",
      "Iter 76700 | D_loss: 1.6817 | G_loss: 0.3760\n",
      "Iter 76800 | D_loss: 1.7483 | G_loss: 0.7652\n",
      "Iter 76900 | D_loss: 1.8608 | G_loss: -0.9436\n",
      "Iter 77000 | D_loss: 1.6486 | G_loss: 0.3884\n",
      "Iter 77100 | D_loss: 1.6725 | G_loss: 0.8390\n",
      "Iter 77200 | D_loss: 1.6997 | G_loss: 0.4172\n",
      "Iter 77300 | D_loss: 2.0782 | G_loss: -0.1389\n",
      "Iter 77400 | D_loss: 1.8172 | G_loss: -0.1695\n",
      "Iter 77500 | D_loss: 1.6154 | G_loss: 0.3331\n",
      "Iter 77600 | D_loss: 1.6979 | G_loss: -0.0680\n",
      "Iter 77700 | D_loss: 1.6550 | G_loss: 0.7305\n",
      "Iter 77800 | D_loss: 1.9126 | G_loss: -0.4047\n",
      "Iter 77900 | D_loss: 2.0954 | G_loss: -0.4336\n",
      "Iter 78000 | D_loss: 1.9509 | G_loss: 1.1500\n",
      "Iter 78100 | D_loss: 1.8343 | G_loss: 0.6337\n",
      "Iter 78200 | D_loss: 1.7132 | G_loss: 0.9716\n",
      "Iter 78300 | D_loss: 1.7036 | G_loss: 0.5765\n",
      "Iter 78400 | D_loss: 1.7550 | G_loss: 0.5099\n",
      "Iter 78500 | D_loss: 1.6931 | G_loss: 0.6225\n",
      "Iter 78600 | D_loss: 2.1025 | G_loss: 0.7790\n",
      "Iter 78700 | D_loss: 1.8421 | G_loss: 0.5476\n",
      "Iter 78800 | D_loss: 1.7421 | G_loss: 0.5892\n",
      "Iter 78900 | D_loss: 1.7049 | G_loss: 0.4588\n",
      "Iter 79000 | D_loss: 1.6770 | G_loss: 0.4668\n",
      "Iter 79100 | D_loss: 1.6424 | G_loss: 0.4965\n",
      "Iter 79200 | D_loss: 1.6790 | G_loss: 0.3665\n",
      "Iter 79300 | D_loss: 1.6815 | G_loss: 0.2781\n",
      "Iter 79400 | D_loss: 1.6678 | G_loss: 0.3901\n",
      "Iter 79500 | D_loss: 1.7520 | G_loss: 0.2738\n",
      "Iter 79600 | D_loss: 1.7180 | G_loss: -0.1882\n",
      "Iter 79700 | D_loss: 1.6270 | G_loss: 0.3156\n",
      "Iter 79800 | D_loss: 2.1917 | G_loss: 1.6623\n",
      "Iter 79900 | D_loss: 1.9043 | G_loss: 0.5037\n",
      "Iter 80000 | D_loss: 1.6793 | G_loss: 0.5340\n",
      "Iter 80100 | D_loss: 1.7506 | G_loss: 0.4875\n",
      "Iter 80200 | D_loss: 1.7037 | G_loss: 0.5519\n",
      "Iter 80300 | D_loss: 1.7053 | G_loss: 0.8756\n",
      "Iter 80400 | D_loss: 1.7601 | G_loss: 0.2023\n",
      "Iter 80500 | D_loss: 1.6697 | G_loss: 0.3201\n",
      "Iter 80600 | D_loss: 1.7393 | G_loss: 0.4670\n",
      "Iter 80700 | D_loss: 1.8020 | G_loss: 0.7193\n",
      "Iter 80800 | D_loss: 1.8077 | G_loss: -0.7821\n",
      "Iter 80900 | D_loss: 2.0147 | G_loss: -0.5556\n",
      "Iter 81000 | D_loss: 1.7162 | G_loss: 0.3667\n",
      "Iter 81100 | D_loss: 1.8195 | G_loss: 0.4183\n",
      "Iter 81200 | D_loss: 1.6447 | G_loss: 0.7236\n",
      "Iter 81300 | D_loss: 1.7331 | G_loss: 0.5758\n",
      "Iter 81400 | D_loss: 1.7391 | G_loss: 0.4710\n",
      "Iter 81500 | D_loss: 1.6890 | G_loss: 0.1322\n",
      "Iter 81600 | D_loss: 1.7509 | G_loss: 0.3753\n",
      "Iter 81700 | D_loss: 2.1114 | G_loss: 0.8630\n",
      "Iter 81800 | D_loss: 1.8274 | G_loss: 0.8907\n",
      "Iter 81900 | D_loss: 2.1498 | G_loss: 1.1385\n",
      "Iter 82000 | D_loss: 1.6936 | G_loss: 0.2509\n",
      "Iter 82100 | D_loss: 1.7576 | G_loss: 0.0605\n",
      "Iter 82200 | D_loss: 1.7232 | G_loss: 0.5042\n",
      "Iter 82300 | D_loss: 1.5940 | G_loss: 0.6164\n",
      "Iter 82400 | D_loss: 1.6427 | G_loss: 0.8273\n",
      "Iter 82500 | D_loss: 1.8001 | G_loss: 0.3570\n",
      "Iter 82600 | D_loss: 1.8826 | G_loss: -0.9496\n",
      "Iter 82700 | D_loss: 1.7805 | G_loss: -0.1615\n",
      "Iter 82800 | D_loss: 1.6857 | G_loss: 0.7409\n",
      "Iter 82900 | D_loss: 1.6992 | G_loss: -0.0491\n",
      "Iter 83000 | D_loss: 1.7221 | G_loss: 0.4883\n",
      "Iter 83100 | D_loss: 1.6993 | G_loss: 0.2362\n",
      "Iter 83200 | D_loss: 1.6697 | G_loss: 0.5017\n",
      "Iter 83300 | D_loss: 1.8658 | G_loss: 0.3226\n",
      "Iter 83400 | D_loss: 1.7265 | G_loss: 0.4230\n",
      "Iter 83500 | D_loss: 1.7056 | G_loss: 0.4291\n",
      "Iter 83600 | D_loss: 1.6559 | G_loss: 0.2347\n",
      "Iter 83700 | D_loss: 1.7792 | G_loss: -0.0986\n",
      "Iter 83800 | D_loss: 1.6570 | G_loss: 0.4807\n",
      "Iter 83900 | D_loss: 1.9264 | G_loss: 1.5078\n",
      "Iter 84000 | D_loss: 2.3242 | G_loss: -0.5232\n",
      "Iter 84100 | D_loss: 1.7909 | G_loss: 0.0223\n",
      "Iter 84200 | D_loss: 1.7302 | G_loss: 0.7407\n",
      "Iter 84300 | D_loss: 1.8254 | G_loss: 0.3153\n",
      "Iter 84400 | D_loss: 1.6901 | G_loss: 0.3191\n",
      "Iter 84500 | D_loss: 2.2532 | G_loss: -0.5237\n",
      "Iter 84600 | D_loss: 1.6118 | G_loss: 0.5645\n",
      "Iter 84700 | D_loss: 1.8986 | G_loss: -1.2798\n",
      "Iter 84800 | D_loss: 1.6889 | G_loss: 0.5226\n",
      "Iter 84900 | D_loss: 1.7126 | G_loss: 0.3307\n",
      "Iter 85000 | D_loss: 1.7032 | G_loss: 0.3121\n",
      "Iter 85100 | D_loss: 1.7185 | G_loss: 0.3882\n",
      "Iter 85200 | D_loss: 1.8791 | G_loss: 0.6396\n",
      "Iter 85300 | D_loss: 1.7384 | G_loss: 0.6154\n",
      "Iter 85400 | D_loss: 1.6819 | G_loss: 0.4521\n",
      "Iter 85500 | D_loss: 1.7473 | G_loss: 0.1916\n",
      "Iter 85600 | D_loss: 1.6118 | G_loss: 0.5095\n",
      "Iter 85700 | D_loss: 1.6535 | G_loss: 0.6493\n",
      "Iter 85800 | D_loss: 2.0035 | G_loss: -0.7769\n",
      "Iter 85900 | D_loss: 1.9988 | G_loss: 1.3827\n",
      "Iter 86000 | D_loss: 1.6911 | G_loss: 0.6848\n",
      "Iter 86100 | D_loss: 1.6914 | G_loss: 0.7738\n",
      "Iter 86200 | D_loss: 1.6749 | G_loss: 0.4895\n",
      "Iter 86300 | D_loss: 2.0289 | G_loss: -0.8032\n",
      "Iter 86400 | D_loss: 1.7376 | G_loss: 0.4491\n",
      "Iter 86500 | D_loss: 1.6846 | G_loss: 0.6354\n",
      "Iter 86600 | D_loss: 1.6816 | G_loss: -0.2106\n",
      "Iter 86700 | D_loss: 1.6914 | G_loss: 0.4757\n",
      "Iter 86800 | D_loss: 1.6970 | G_loss: 0.7940\n",
      "Iter 86900 | D_loss: 1.7228 | G_loss: -0.0061\n",
      "Iter 87000 | D_loss: 1.7302 | G_loss: 0.3378\n",
      "Iter 87100 | D_loss: 1.7268 | G_loss: 0.8410\n",
      "Iter 87200 | D_loss: 1.6660 | G_loss: -0.3281\n",
      "Iter 87300 | D_loss: 1.7150 | G_loss: 0.3832\n",
      "Iter 87400 | D_loss: 1.5766 | G_loss: 0.6435\n",
      "Iter 87500 | D_loss: 1.8570 | G_loss: -0.5384\n",
      "Iter 87600 | D_loss: 1.6710 | G_loss: 0.5241\n",
      "Iter 87700 | D_loss: 1.6061 | G_loss: 0.5624\n",
      "Iter 87800 | D_loss: 1.7365 | G_loss: 0.4679\n",
      "Iter 87900 | D_loss: 1.6966 | G_loss: 0.3625\n",
      "Iter 88000 | D_loss: 1.6747 | G_loss: 0.6508\n",
      "Iter 88100 | D_loss: 1.8171 | G_loss: 0.4940\n",
      "Iter 88200 | D_loss: 1.5718 | G_loss: 0.5707\n",
      "Iter 88300 | D_loss: 1.7963 | G_loss: 0.7508\n",
      "Iter 88400 | D_loss: 1.7703 | G_loss: 0.5057\n",
      "Iter 88500 | D_loss: 1.7554 | G_loss: 0.6795\n",
      "Iter 88600 | D_loss: 2.3000 | G_loss: -0.7478\n",
      "Iter 88700 | D_loss: 1.7322 | G_loss: 0.8046\n",
      "Iter 88800 | D_loss: 1.5666 | G_loss: 0.4567\n",
      "Iter 88900 | D_loss: 1.6646 | G_loss: 0.6375\n",
      "Iter 89000 | D_loss: 1.6218 | G_loss: 0.6742\n",
      "Iter 89100 | D_loss: 1.8378 | G_loss: 0.2119\n",
      "Iter 89200 | D_loss: 1.6790 | G_loss: -0.5207\n",
      "Iter 89300 | D_loss: 1.5754 | G_loss: 0.5670\n",
      "Iter 89400 | D_loss: 1.6681 | G_loss: 0.7061\n",
      "Iter 89500 | D_loss: 1.7722 | G_loss: 0.6599\n",
      "Iter 89600 | D_loss: 1.6222 | G_loss: 0.2816\n",
      "Iter 89700 | D_loss: 1.6941 | G_loss: 0.3645\n",
      "Iter 89800 | D_loss: 1.7215 | G_loss: 0.5514\n",
      "Iter 89900 | D_loss: 1.5781 | G_loss: 0.5228\n",
      "Iter 90000 | D_loss: 1.5990 | G_loss: 0.6391\n",
      "Iter 90100 | D_loss: 1.6956 | G_loss: 0.5006\n",
      "Iter 90200 | D_loss: 1.8285 | G_loss: 0.4026\n",
      "Iter 90300 | D_loss: 1.6976 | G_loss: 0.1928\n",
      "Iter 90400 | D_loss: 1.7606 | G_loss: 0.6196\n",
      "Iter 90500 | D_loss: 1.5675 | G_loss: 0.3647\n",
      "Iter 90600 | D_loss: 1.6355 | G_loss: 0.6091\n",
      "Iter 90700 | D_loss: 1.7179 | G_loss: 0.5045\n",
      "Iter 90800 | D_loss: 1.6626 | G_loss: 0.3510\n",
      "Iter 90900 | D_loss: 1.7172 | G_loss: 0.0806\n",
      "Iter 91000 | D_loss: 1.6940 | G_loss: 0.3474\n",
      "Iter 91100 | D_loss: 1.7008 | G_loss: 1.0164\n",
      "Iter 91200 | D_loss: 1.6458 | G_loss: 0.6701\n",
      "Iter 91300 | D_loss: 1.7564 | G_loss: 0.6818\n",
      "Iter 91400 | D_loss: 1.9082 | G_loss: 0.4698\n",
      "Iter 91500 | D_loss: 1.6040 | G_loss: 0.1299\n",
      "Iter 91600 | D_loss: 1.6704 | G_loss: 0.4443\n",
      "Iter 91700 | D_loss: 1.6524 | G_loss: 0.5925\n",
      "Iter 91800 | D_loss: 1.6413 | G_loss: 0.9924\n",
      "Iter 91900 | D_loss: 1.6854 | G_loss: 0.6633\n",
      "Iter 92000 | D_loss: 1.6611 | G_loss: 0.5667\n",
      "Iter 92100 | D_loss: 1.6675 | G_loss: 0.6055\n",
      "Iter 92200 | D_loss: 1.5689 | G_loss: 0.4908\n",
      "Iter 92300 | D_loss: 1.7370 | G_loss: 0.1798\n",
      "Iter 92400 | D_loss: 1.7145 | G_loss: 0.3812\n",
      "Iter 92500 | D_loss: 1.6820 | G_loss: 0.4818\n",
      "Iter 92600 | D_loss: 1.6992 | G_loss: 0.4827\n",
      "Iter 92700 | D_loss: 1.6205 | G_loss: 0.1819\n",
      "Iter 92800 | D_loss: 1.5899 | G_loss: 0.8786\n",
      "Iter 92900 | D_loss: 1.6647 | G_loss: 0.3416\n",
      "Iter 93000 | D_loss: 1.6676 | G_loss: 0.6280\n",
      "Iter 93100 | D_loss: 1.7484 | G_loss: 0.6518\n",
      "Iter 93200 | D_loss: 1.6617 | G_loss: 0.2537\n",
      "Iter 93300 | D_loss: 1.7406 | G_loss: 0.4881\n",
      "Iter 93400 | D_loss: 1.6214 | G_loss: 0.0771\n",
      "Iter 93500 | D_loss: 1.6806 | G_loss: 0.4924\n",
      "Iter 93600 | D_loss: 1.6206 | G_loss: 0.3806\n",
      "Iter 93700 | D_loss: 1.6872 | G_loss: 0.4762\n",
      "Iter 93800 | D_loss: 1.7317 | G_loss: 0.5264\n",
      "Iter 93900 | D_loss: 1.6674 | G_loss: 0.3391\n",
      "Iter 94000 | D_loss: 1.7257 | G_loss: 0.5661\n",
      "Iter 94100 | D_loss: 1.7311 | G_loss: 0.9919\n",
      "Iter 94200 | D_loss: 1.6910 | G_loss: 0.4037\n",
      "Iter 94300 | D_loss: 1.7010 | G_loss: 0.2885\n",
      "Iter 94400 | D_loss: 1.6803 | G_loss: -0.4392\n",
      "Iter 94500 | D_loss: 1.6990 | G_loss: 0.5486\n",
      "Iter 94600 | D_loss: 1.6994 | G_loss: 0.5206\n",
      "Iter 94700 | D_loss: 1.6946 | G_loss: 0.3006\n",
      "Iter 94800 | D_loss: 1.6996 | G_loss: 0.4772\n",
      "Iter 94900 | D_loss: 1.7362 | G_loss: -1.2298\n",
      "Iter 95000 | D_loss: 1.6902 | G_loss: 0.4831\n",
      "Iter 95100 | D_loss: 1.7624 | G_loss: -0.9323\n",
      "Iter 95200 | D_loss: 1.6437 | G_loss: 0.2717\n",
      "Iter 95300 | D_loss: 1.7468 | G_loss: 0.6051\n",
      "Iter 95400 | D_loss: 1.6933 | G_loss: 0.6890\n",
      "Iter 95500 | D_loss: 1.7359 | G_loss: 0.4565\n",
      "Iter 95600 | D_loss: 1.9136 | G_loss: 0.5060\n",
      "Iter 95700 | D_loss: 1.6458 | G_loss: 0.4034\n",
      "Iter 95800 | D_loss: 1.8356 | G_loss: 0.3125\n",
      "Iter 95900 | D_loss: 1.8020 | G_loss: 0.4692\n",
      "Iter 96000 | D_loss: 2.1649 | G_loss: 1.2593\n",
      "Iter 96100 | D_loss: 1.7095 | G_loss: 0.2845\n",
      "Iter 96200 | D_loss: 1.6152 | G_loss: 0.3929\n",
      "Iter 96300 | D_loss: 1.6164 | G_loss: 0.4530\n",
      "Iter 96400 | D_loss: 1.5315 | G_loss: 0.4612\n",
      "Iter 96500 | D_loss: 1.5508 | G_loss: 0.3950\n",
      "Iter 96600 | D_loss: 1.7207 | G_loss: 0.5340\n",
      "Iter 96700 | D_loss: 1.6377 | G_loss: -0.3038\n",
      "Iter 96800 | D_loss: 1.6420 | G_loss: 0.6038\n",
      "Iter 96900 | D_loss: 1.8261 | G_loss: 0.6255\n",
      "Iter 97000 | D_loss: 1.9550 | G_loss: -1.0325\n",
      "Iter 97100 | D_loss: 1.5696 | G_loss: 0.4075\n",
      "Iter 97200 | D_loss: 1.6162 | G_loss: 0.4386\n",
      "Iter 97300 | D_loss: 1.6884 | G_loss: 0.3654\n",
      "Iter 97400 | D_loss: 1.8152 | G_loss: 0.3673\n",
      "Iter 97500 | D_loss: 1.6993 | G_loss: 0.2398\n",
      "Iter 97600 | D_loss: 1.7382 | G_loss: 0.3098\n",
      "Iter 97700 | D_loss: 1.7267 | G_loss: 0.2052\n",
      "Iter 97800 | D_loss: 2.4915 | G_loss: -0.4781\n",
      "Iter 97900 | D_loss: 1.7297 | G_loss: 1.1310\n",
      "Iter 98000 | D_loss: 1.6664 | G_loss: 0.4727\n",
      "Iter 98100 | D_loss: 1.7508 | G_loss: 0.7029\n",
      "Iter 98200 | D_loss: 1.6567 | G_loss: 0.8107\n",
      "Iter 98300 | D_loss: 1.6197 | G_loss: 0.3590\n",
      "Iter 98400 | D_loss: 1.6950 | G_loss: 0.4057\n",
      "Iter 98500 | D_loss: 1.7202 | G_loss: 0.4289\n",
      "Iter 98600 | D_loss: 1.6578 | G_loss: 0.3289\n",
      "Iter 98700 | D_loss: 1.5739 | G_loss: 0.6238\n",
      "Iter 98800 | D_loss: 1.6377 | G_loss: 0.4280\n",
      "Iter 98900 | D_loss: 1.6461 | G_loss: 0.4387\n",
      "Iter 99000 | D_loss: 1.7161 | G_loss: 0.6090\n",
      "Iter 99100 | D_loss: 1.5441 | G_loss: 0.1102\n",
      "Iter 99200 | D_loss: 1.6600 | G_loss: 0.3115\n",
      "Iter 99300 | D_loss: 1.6355 | G_loss: 0.6051\n",
      "Iter 99400 | D_loss: 1.7127 | G_loss: 0.3546\n",
      "Iter 99500 | D_loss: 2.1786 | G_loss: -0.4422\n",
      "Iter 99600 | D_loss: 1.6422 | G_loss: 0.4611\n",
      "Iter 99700 | D_loss: 1.5821 | G_loss: 0.6226\n",
      "Iter 99800 | D_loss: 1.7631 | G_loss: 0.1121\n",
      "Iter 99900 | D_loss: 1.7423 | G_loss: 0.2939\n",
      "Iter 100000 | D_loss: 1.6708 | G_loss: 0.4263\n",
      "Iter 100100 | D_loss: 1.5390 | G_loss: 0.2259\n",
      "Iter 100200 | D_loss: 2.1692 | G_loss: 1.3497\n",
      "Iter 100300 | D_loss: 1.6791 | G_loss: 0.2858\n",
      "Iter 100400 | D_loss: 1.6455 | G_loss: -0.3972\n",
      "Iter 100500 | D_loss: 1.6162 | G_loss: 0.6998\n",
      "Iter 100600 | D_loss: 1.5292 | G_loss: 0.5685\n",
      "Iter 100700 | D_loss: 1.5808 | G_loss: 0.2474\n",
      "Iter 100800 | D_loss: 1.6923 | G_loss: 0.6465\n",
      "Iter 100900 | D_loss: 1.6422 | G_loss: 0.5402\n",
      "Iter 101000 | D_loss: 1.7154 | G_loss: 0.4797\n",
      "Iter 101100 | D_loss: 1.7302 | G_loss: 0.1039\n",
      "Iter 101200 | D_loss: 1.6032 | G_loss: 0.3017\n",
      "Iter 101300 | D_loss: 1.7306 | G_loss: 0.5099\n",
      "Iter 101400 | D_loss: 1.8932 | G_loss: 0.5766\n",
      "Iter 101500 | D_loss: 1.7793 | G_loss: 0.4263\n",
      "Iter 101600 | D_loss: 1.6957 | G_loss: 0.6213\n",
      "Iter 101700 | D_loss: 1.8293 | G_loss: 0.6922\n",
      "Iter 101800 | D_loss: 1.8981 | G_loss: 0.3639\n",
      "Iter 101900 | D_loss: 1.7380 | G_loss: 0.9564\n",
      "Iter 102000 | D_loss: 1.7122 | G_loss: 0.0276\n",
      "Iter 102100 | D_loss: 1.6827 | G_loss: 0.5292\n",
      "Iter 102200 | D_loss: 1.7882 | G_loss: 0.3927\n",
      "Iter 102300 | D_loss: 1.7051 | G_loss: 0.4219\n",
      "Iter 102400 | D_loss: 1.6092 | G_loss: 0.5572\n",
      "Iter 102500 | D_loss: 1.6904 | G_loss: 0.2680\n",
      "Iter 102600 | D_loss: 2.3496 | G_loss: 1.3872\n",
      "Iter 102700 | D_loss: 1.6811 | G_loss: 0.4503\n",
      "Iter 102800 | D_loss: 1.7311 | G_loss: -0.4129\n",
      "Iter 102900 | D_loss: 1.7129 | G_loss: 0.4694\n",
      "Iter 103000 | D_loss: 1.5844 | G_loss: 0.2079\n",
      "Iter 103100 | D_loss: 1.7195 | G_loss: 0.4186\n",
      "Iter 103200 | D_loss: 1.6586 | G_loss: 0.9396\n",
      "Iter 103300 | D_loss: 1.9955 | G_loss: -0.4049\n",
      "Iter 103400 | D_loss: 1.7738 | G_loss: 1.2768\n",
      "Iter 103500 | D_loss: 1.6540 | G_loss: 0.4877\n",
      "Iter 103600 | D_loss: 1.6996 | G_loss: 1.1963\n",
      "Iter 103700 | D_loss: 1.5884 | G_loss: 0.7113\n",
      "Iter 103800 | D_loss: 1.7855 | G_loss: -0.2946\n",
      "Iter 103900 | D_loss: 1.6637 | G_loss: 0.5494\n",
      "Iter 104000 | D_loss: 1.7220 | G_loss: 0.5022\n",
      "Iter 104100 | D_loss: 1.7441 | G_loss: 0.4366\n",
      "Iter 104200 | D_loss: 1.6035 | G_loss: 0.0803\n",
      "Iter 104300 | D_loss: 1.6315 | G_loss: 0.6262\n",
      "Iter 104400 | D_loss: 1.6288 | G_loss: 0.4250\n",
      "Iter 104500 | D_loss: 1.6916 | G_loss: 0.4147\n",
      "Iter 104600 | D_loss: 1.6672 | G_loss: -0.2072\n",
      "Iter 104700 | D_loss: 2.0331 | G_loss: 1.1426\n",
      "Iter 104800 | D_loss: 1.6605 | G_loss: 0.4376\n",
      "Iter 104900 | D_loss: 1.7469 | G_loss: 0.4945\n",
      "Iter 105000 | D_loss: 1.5829 | G_loss: 0.4938\n",
      "Iter 105100 | D_loss: 1.6623 | G_loss: 0.3572\n",
      "Iter 105200 | D_loss: 1.5933 | G_loss: 0.4072\n",
      "Iter 105300 | D_loss: 2.3359 | G_loss: 1.0973\n",
      "Iter 105400 | D_loss: 1.7263 | G_loss: 0.4411\n",
      "Iter 105500 | D_loss: 1.6939 | G_loss: 0.7759\n",
      "Iter 105600 | D_loss: 1.6957 | G_loss: 0.3993\n",
      "Iter 105700 | D_loss: 1.6748 | G_loss: 0.6356\n",
      "Iter 105800 | D_loss: 1.7170 | G_loss: -0.9956\n",
      "Iter 105900 | D_loss: 1.7053 | G_loss: 0.7276\n",
      "Iter 106000 | D_loss: 1.6133 | G_loss: -0.2739\n",
      "Iter 106100 | D_loss: 1.7225 | G_loss: 0.7357\n",
      "Iter 106200 | D_loss: 1.6514 | G_loss: 0.3988\n",
      "Iter 106300 | D_loss: 1.7647 | G_loss: 0.8911\n",
      "Iter 106400 | D_loss: 1.6495 | G_loss: 0.7369\n",
      "Iter 106500 | D_loss: 1.7291 | G_loss: 0.3960\n",
      "Iter 106600 | D_loss: 1.6516 | G_loss: 0.3834\n",
      "Iter 106700 | D_loss: 1.7697 | G_loss: 0.1120\n",
      "Iter 106800 | D_loss: 1.8021 | G_loss: 0.9508\n",
      "Iter 106900 | D_loss: 1.6937 | G_loss: 0.4315\n",
      "Iter 107000 | D_loss: 1.6964 | G_loss: 0.4905\n",
      "Iter 107100 | D_loss: 1.6753 | G_loss: 0.1861\n",
      "Iter 107200 | D_loss: 1.6435 | G_loss: -0.1225\n",
      "Iter 107300 | D_loss: 1.7115 | G_loss: 0.4497\n",
      "Iter 107400 | D_loss: 1.7215 | G_loss: 0.8914\n",
      "Iter 107500 | D_loss: 1.7667 | G_loss: 0.7846\n",
      "Iter 107600 | D_loss: 2.1431 | G_loss: -0.4584\n",
      "Iter 107700 | D_loss: 1.6749 | G_loss: 0.6410\n",
      "Iter 107800 | D_loss: 1.6691 | G_loss: 0.2194\n",
      "Iter 107900 | D_loss: 1.5868 | G_loss: 1.1661\n",
      "Iter 108000 | D_loss: 1.6612 | G_loss: 0.4147\n",
      "Iter 108100 | D_loss: 1.6413 | G_loss: 0.2124\n",
      "Iter 108200 | D_loss: 1.6964 | G_loss: 1.2925\n",
      "Iter 108300 | D_loss: 1.6409 | G_loss: 0.4089\n",
      "Iter 108400 | D_loss: 1.6390 | G_loss: 0.4179\n",
      "Iter 108500 | D_loss: 1.6608 | G_loss: 0.4802\n",
      "Iter 108600 | D_loss: 1.6483 | G_loss: 0.5463\n",
      "Iter 108700 | D_loss: 1.5694 | G_loss: 0.5620\n",
      "Iter 108800 | D_loss: 1.5784 | G_loss: 0.9351\n",
      "Iter 108900 | D_loss: 1.6511 | G_loss: 0.3731\n",
      "Iter 109000 | D_loss: 1.6095 | G_loss: 0.1503\n",
      "Iter 109100 | D_loss: 1.7902 | G_loss: 1.2712\n",
      "Iter 109200 | D_loss: 1.6532 | G_loss: 0.6671\n",
      "Iter 109300 | D_loss: 2.0128 | G_loss: 1.4588\n",
      "Iter 109400 | D_loss: 1.6244 | G_loss: 0.9569\n",
      "Iter 109500 | D_loss: 1.7276 | G_loss: 0.4002\n",
      "Iter 109600 | D_loss: 1.6938 | G_loss: 0.4209\n",
      "Iter 109700 | D_loss: 1.6754 | G_loss: 0.0277\n",
      "Iter 109800 | D_loss: 1.5709 | G_loss: 0.5915\n",
      "Iter 109900 | D_loss: 1.8883 | G_loss: -0.7802\n",
      "Iter 110000 | D_loss: 1.5841 | G_loss: 0.5009\n",
      "Iter 110100 | D_loss: 1.6610 | G_loss: 0.1621\n",
      "Iter 110200 | D_loss: 1.5647 | G_loss: 0.4781\n",
      "Iter 110300 | D_loss: 1.5690 | G_loss: 0.4001\n",
      "Iter 110400 | D_loss: 1.6715 | G_loss: -0.5910\n",
      "Iter 110500 | D_loss: 1.7730 | G_loss: -1.2415\n",
      "Iter 110600 | D_loss: 1.6314 | G_loss: 0.4987\n",
      "Iter 110700 | D_loss: 1.7124 | G_loss: 0.4075\n",
      "Iter 110800 | D_loss: 1.7636 | G_loss: 0.6995\n",
      "Iter 110900 | D_loss: 1.6493 | G_loss: 0.2407\n",
      "Iter 111000 | D_loss: 1.6368 | G_loss: 0.6664\n",
      "Iter 111100 | D_loss: 1.5264 | G_loss: 0.6333\n",
      "Iter 111200 | D_loss: 1.6769 | G_loss: 0.5098\n",
      "Iter 111300 | D_loss: 1.6467 | G_loss: -0.6587\n",
      "Iter 111400 | D_loss: 1.7285 | G_loss: 0.4701\n",
      "Iter 111500 | D_loss: 1.7922 | G_loss: 0.7325\n",
      "Iter 111600 | D_loss: 1.8683 | G_loss: -0.4921\n",
      "Iter 111700 | D_loss: 1.5657 | G_loss: 0.1002\n",
      "Iter 111800 | D_loss: 1.6678 | G_loss: 0.3352\n",
      "Iter 111900 | D_loss: 1.6461 | G_loss: 0.3716\n",
      "Iter 112000 | D_loss: 1.7996 | G_loss: 0.2723\n",
      "Iter 112100 | D_loss: 1.8089 | G_loss: 0.0716\n",
      "Iter 112200 | D_loss: 1.7430 | G_loss: 0.8503\n",
      "Iter 112300 | D_loss: 1.6108 | G_loss: 0.1252\n",
      "Iter 112400 | D_loss: 1.6929 | G_loss: 0.5515\n",
      "Iter 112500 | D_loss: 1.5859 | G_loss: 0.7256\n",
      "Iter 112600 | D_loss: 1.7494 | G_loss: -0.0954\n",
      "Iter 112700 | D_loss: 1.5718 | G_loss: 0.5097\n",
      "Iter 112800 | D_loss: 1.6938 | G_loss: 0.6244\n",
      "Iter 112900 | D_loss: 1.9403 | G_loss: 0.7484\n",
      "Iter 113000 | D_loss: 2.3241 | G_loss: -0.5303\n",
      "Iter 113100 | D_loss: 1.6531 | G_loss: 0.2739\n",
      "Iter 113200 | D_loss: 1.6456 | G_loss: 0.8801\n",
      "Iter 113300 | D_loss: 1.6416 | G_loss: 0.6344\n",
      "Iter 113400 | D_loss: 1.6217 | G_loss: 0.4527\n",
      "Iter 113500 | D_loss: 1.6371 | G_loss: -0.6252\n",
      "Iter 113600 | D_loss: 2.0120 | G_loss: 0.5858\n",
      "Iter 113700 | D_loss: 1.5793 | G_loss: 0.1552\n",
      "Iter 113800 | D_loss: 1.6363 | G_loss: 0.0830\n",
      "Iter 113900 | D_loss: 1.6967 | G_loss: 0.3918\n",
      "Iter 114000 | D_loss: 1.7059 | G_loss: 0.5715\n",
      "Iter 114100 | D_loss: 1.6501 | G_loss: 0.3300\n",
      "Iter 114200 | D_loss: 1.6264 | G_loss: 1.0629\n",
      "Iter 114300 | D_loss: 2.0448 | G_loss: -0.8392\n",
      "Iter 114400 | D_loss: 1.5942 | G_loss: 0.5255\n",
      "Iter 114500 | D_loss: 1.7587 | G_loss: -0.0980\n",
      "Iter 114600 | D_loss: 1.5748 | G_loss: -0.1013\n",
      "Iter 114700 | D_loss: 1.6073 | G_loss: 1.1544\n",
      "Iter 114800 | D_loss: 1.7281 | G_loss: -0.1353\n",
      "Iter 114900 | D_loss: 1.5914 | G_loss: 0.3423\n",
      "Iter 115000 | D_loss: 1.6446 | G_loss: 0.6696\n",
      "Iter 115100 | D_loss: 1.6638 | G_loss: 0.1527\n",
      "Iter 115200 | D_loss: 1.7784 | G_loss: 0.5061\n",
      "Iter 115300 | D_loss: 1.6008 | G_loss: 0.2951\n",
      "Iter 115400 | D_loss: 1.5384 | G_loss: 0.4941\n",
      "Iter 115500 | D_loss: 1.6979 | G_loss: 0.3683\n",
      "Iter 115600 | D_loss: 1.5865 | G_loss: 0.6566\n",
      "Iter 115700 | D_loss: 1.6369 | G_loss: 0.8176\n",
      "Iter 115800 | D_loss: 1.5924 | G_loss: 0.4150\n",
      "Iter 115900 | D_loss: 1.7710 | G_loss: -0.8294\n",
      "Iter 116000 | D_loss: 1.5310 | G_loss: 0.2143\n",
      "Iter 116100 | D_loss: 1.6901 | G_loss: -0.9902\n",
      "Iter 116200 | D_loss: 1.6464 | G_loss: 0.8175\n",
      "Iter 116300 | D_loss: 1.7189 | G_loss: 0.2558\n",
      "Iter 116400 | D_loss: 1.7671 | G_loss: 0.2185\n",
      "Iter 116500 | D_loss: 1.6576 | G_loss: 0.4096\n",
      "Iter 116600 | D_loss: 1.6467 | G_loss: 1.1313\n",
      "Iter 116700 | D_loss: 1.4388 | G_loss: 0.7932\n",
      "Iter 116800 | D_loss: 1.7314 | G_loss: 0.5837\n",
      "Iter 116900 | D_loss: 1.7581 | G_loss: 1.2349\n",
      "Iter 117000 | D_loss: 1.5139 | G_loss: 0.7877\n",
      "Iter 117100 | D_loss: 1.6405 | G_loss: 0.3363\n",
      "Iter 117200 | D_loss: 1.6528 | G_loss: 0.2207\n",
      "Iter 117300 | D_loss: 1.6658 | G_loss: 0.5312\n",
      "Iter 117400 | D_loss: 1.7513 | G_loss: 0.2025\n",
      "Iter 117500 | D_loss: 1.8641 | G_loss: 1.6729\n",
      "Iter 117600 | D_loss: 1.6916 | G_loss: 0.3012\n",
      "Iter 117700 | D_loss: 1.5808 | G_loss: 0.5418\n",
      "Iter 117800 | D_loss: 1.6522 | G_loss: 0.4855\n",
      "Iter 117900 | D_loss: 1.7478 | G_loss: 0.3002\n",
      "Iter 118000 | D_loss: 1.7068 | G_loss: 0.5014\n",
      "Iter 118100 | D_loss: 1.6451 | G_loss: 0.0266\n",
      "Iter 118200 | D_loss: 1.6441 | G_loss: 0.7163\n",
      "Iter 118300 | D_loss: 1.6489 | G_loss: 0.2887\n",
      "Iter 118400 | D_loss: 1.5742 | G_loss: 0.5208\n",
      "Iter 118500 | D_loss: 1.5831 | G_loss: 0.8914\n",
      "Iter 118600 | D_loss: 1.6805 | G_loss: 0.9606\n",
      "Iter 118700 | D_loss: 1.5773 | G_loss: 0.4646\n",
      "Iter 118800 | D_loss: 1.6361 | G_loss: 0.5153\n",
      "Iter 118900 | D_loss: 1.6916 | G_loss: 0.3509\n",
      "Iter 119000 | D_loss: 1.6904 | G_loss: 0.6044\n",
      "Iter 119100 | D_loss: 1.6052 | G_loss: 0.3192\n",
      "Iter 119200 | D_loss: 1.4692 | G_loss: 0.6923\n",
      "Iter 119300 | D_loss: 1.6779 | G_loss: 0.4217\n",
      "Iter 119400 | D_loss: 1.7176 | G_loss: 0.3782\n",
      "Iter 119500 | D_loss: 1.7067 | G_loss: 0.0501\n",
      "Iter 119600 | D_loss: 1.6994 | G_loss: -0.2702\n",
      "Iter 119700 | D_loss: 1.7356 | G_loss: -0.4127\n",
      "Iter 119800 | D_loss: 1.6692 | G_loss: 0.7558\n",
      "Iter 119900 | D_loss: 1.6494 | G_loss: 0.4353\n",
      "Iter 120000 | D_loss: 1.6404 | G_loss: 0.3862\n",
      "Iter 120100 | D_loss: 1.6084 | G_loss: 0.7501\n",
      "Iter 120200 | D_loss: 1.5902 | G_loss: 0.6980\n",
      "Iter 120300 | D_loss: 1.5269 | G_loss: 0.2843\n",
      "Iter 120400 | D_loss: 1.6470 | G_loss: -0.1710\n",
      "Iter 120500 | D_loss: 1.5739 | G_loss: 0.6113\n",
      "Iter 120600 | D_loss: 1.7393 | G_loss: 0.6704\n",
      "Iter 120700 | D_loss: 1.6512 | G_loss: 0.2891\n",
      "Iter 120800 | D_loss: 1.6876 | G_loss: 0.0493\n",
      "Iter 120900 | D_loss: 1.7071 | G_loss: 0.6193\n",
      "Iter 121000 | D_loss: 1.6709 | G_loss: 0.6465\n",
      "Iter 121100 | D_loss: 1.5793 | G_loss: 0.5038\n",
      "Iter 121200 | D_loss: 1.6263 | G_loss: 0.2638\n",
      "Iter 121300 | D_loss: 1.5730 | G_loss: 0.4680\n",
      "Iter 121400 | D_loss: 1.5721 | G_loss: 0.3256\n",
      "Iter 121500 | D_loss: 1.6515 | G_loss: 0.6011\n",
      "Iter 121600 | D_loss: 1.7258 | G_loss: 0.3804\n",
      "Iter 121700 | D_loss: 1.5201 | G_loss: 0.7520\n",
      "Iter 121800 | D_loss: 1.7039 | G_loss: 0.4708\n",
      "Iter 121900 | D_loss: 1.5539 | G_loss: 0.1330\n",
      "Iter 122000 | D_loss: 1.6686 | G_loss: 0.3989\n",
      "Iter 122100 | D_loss: 1.6332 | G_loss: 0.5646\n",
      "Iter 122200 | D_loss: 1.5592 | G_loss: 0.5541\n",
      "Iter 122300 | D_loss: 1.7135 | G_loss: 0.6195\n",
      "Iter 122400 | D_loss: 1.8118 | G_loss: -0.6398\n",
      "Iter 122500 | D_loss: 1.5625 | G_loss: 0.4399\n",
      "Iter 122600 | D_loss: 1.6489 | G_loss: 0.9645\n",
      "Iter 122700 | D_loss: 1.6119 | G_loss: 0.5982\n",
      "Iter 122800 | D_loss: 2.0527 | G_loss: 0.9055\n",
      "Iter 122900 | D_loss: 1.6254 | G_loss: 0.2814\n",
      "Iter 123000 | D_loss: 1.5540 | G_loss: 0.9779\n",
      "Iter 123100 | D_loss: 1.7008 | G_loss: 0.5249\n",
      "Iter 123200 | D_loss: 1.5530 | G_loss: 0.4591\n",
      "Iter 123300 | D_loss: 1.7872 | G_loss: 0.7797\n",
      "Iter 123400 | D_loss: 1.6369 | G_loss: -0.1752\n",
      "Iter 123500 | D_loss: 1.6428 | G_loss: 0.5548\n",
      "Iter 123600 | D_loss: 1.7446 | G_loss: 0.4784\n",
      "Iter 123700 | D_loss: 1.6630 | G_loss: 0.2573\n",
      "Iter 123800 | D_loss: 1.5989 | G_loss: 0.4785\n",
      "Iter 123900 | D_loss: 1.6790 | G_loss: 0.0066\n",
      "Iter 124000 | D_loss: 1.6890 | G_loss: -0.1306\n",
      "Iter 124100 | D_loss: 1.6480 | G_loss: 0.3041\n",
      "Iter 124200 | D_loss: 1.6544 | G_loss: 0.4745\n",
      "Iter 124300 | D_loss: 1.5993 | G_loss: 0.3389\n",
      "Iter 124400 | D_loss: 1.5740 | G_loss: 0.2250\n",
      "Iter 124500 | D_loss: 1.6891 | G_loss: -0.1431\n",
      "Iter 124600 | D_loss: 1.7315 | G_loss: 0.4684\n",
      "Iter 124700 | D_loss: 1.6319 | G_loss: 0.9560\n",
      "Iter 124800 | D_loss: 1.6942 | G_loss: -0.1351\n",
      "Iter 124900 | D_loss: 1.5578 | G_loss: 0.5942\n",
      "Iter 125000 | D_loss: 1.6842 | G_loss: -0.2161\n",
      "Iter 125100 | D_loss: 1.7287 | G_loss: 0.4986\n",
      "Iter 125200 | D_loss: 1.6156 | G_loss: 0.5253\n",
      "Iter 125300 | D_loss: 1.6670 | G_loss: 0.6916\n",
      "Iter 125400 | D_loss: 1.5366 | G_loss: 0.4016\n",
      "Iter 125500 | D_loss: 1.5760 | G_loss: 0.5654\n",
      "Iter 125600 | D_loss: 1.9132 | G_loss: -1.1477\n",
      "Iter 125700 | D_loss: 1.5828 | G_loss: 0.6124\n",
      "Iter 125800 | D_loss: 1.6248 | G_loss: 0.6909\n",
      "Iter 125900 | D_loss: 2.0517 | G_loss: -0.9319\n",
      "Iter 126000 | D_loss: 1.6664 | G_loss: 0.3694\n",
      "Iter 126100 | D_loss: 1.7508 | G_loss: 1.4592\n",
      "Iter 126200 | D_loss: 1.5835 | G_loss: -0.4253\n",
      "Iter 126300 | D_loss: 1.6154 | G_loss: 0.6476\n",
      "Iter 126400 | D_loss: 1.9392 | G_loss: -0.5903\n",
      "Iter 126500 | D_loss: 1.5403 | G_loss: 0.3479\n",
      "Iter 126600 | D_loss: 1.6408 | G_loss: 0.4833\n",
      "Iter 126700 | D_loss: 1.5190 | G_loss: 0.2871\n",
      "Iter 126800 | D_loss: 1.6071 | G_loss: 0.4202\n",
      "Iter 126900 | D_loss: 1.5313 | G_loss: 0.5378\n",
      "Iter 127000 | D_loss: 1.5934 | G_loss: 1.4648\n",
      "Iter 127100 | D_loss: 1.5791 | G_loss: 0.8423\n",
      "Iter 127200 | D_loss: 1.6686 | G_loss: 0.3786\n",
      "Iter 127300 | D_loss: 1.6182 | G_loss: 0.2293\n",
      "Iter 127400 | D_loss: 1.7088 | G_loss: 0.3962\n",
      "Iter 127500 | D_loss: 1.5571 | G_loss: 0.4275\n",
      "Iter 127600 | D_loss: 1.5443 | G_loss: 0.3789\n",
      "Iter 127700 | D_loss: 2.3289 | G_loss: -0.2080\n",
      "Iter 127800 | D_loss: 1.6354 | G_loss: 0.3018\n",
      "Iter 127900 | D_loss: 1.6170 | G_loss: 0.5905\n",
      "Iter 128000 | D_loss: 1.5691 | G_loss: -0.5792\n",
      "Iter 128100 | D_loss: 1.6507 | G_loss: 0.5427\n",
      "Iter 128200 | D_loss: 1.6316 | G_loss: 0.4125\n",
      "Iter 128300 | D_loss: 1.5812 | G_loss: 0.5896\n",
      "Iter 128400 | D_loss: 1.6464 | G_loss: 0.2320\n",
      "Iter 128500 | D_loss: 1.6327 | G_loss: 0.3985\n",
      "Iter 128600 | D_loss: 1.5691 | G_loss: 0.4334\n",
      "Iter 128700 | D_loss: 1.6632 | G_loss: 0.5015\n",
      "Iter 128800 | D_loss: 1.6576 | G_loss: 0.1768\n",
      "Iter 128900 | D_loss: 1.6132 | G_loss: 0.4474\n",
      "Iter 129000 | D_loss: 1.6288 | G_loss: 0.3233\n",
      "Iter 129100 | D_loss: 1.7947 | G_loss: 0.5812\n",
      "Iter 129200 | D_loss: 1.6699 | G_loss: 0.6857\n",
      "Iter 129300 | D_loss: 1.6880 | G_loss: 0.3573\n",
      "Iter 129400 | D_loss: 1.6306 | G_loss: -0.6709\n",
      "Iter 129500 | D_loss: 1.5345 | G_loss: 0.4364\n",
      "Iter 129600 | D_loss: 1.6506 | G_loss: 0.7731\n",
      "Iter 129700 | D_loss: 1.6628 | G_loss: 0.5232\n",
      "Iter 129800 | D_loss: 1.6038 | G_loss: 0.6743\n",
      "Iter 129900 | D_loss: 1.4850 | G_loss: 0.7339\n",
      "Iter 130000 | D_loss: 1.6222 | G_loss: 0.3926\n",
      "Iter 130100 | D_loss: 1.6678 | G_loss: 0.4005\n",
      "Iter 130200 | D_loss: 1.5698 | G_loss: 0.8286\n",
      "Iter 130300 | D_loss: 1.5553 | G_loss: 0.3317\n",
      "Iter 130400 | D_loss: 1.6103 | G_loss: 0.4463\n",
      "Iter 130500 | D_loss: 1.5975 | G_loss: 0.4523\n",
      "Iter 130600 | D_loss: 1.5772 | G_loss: 0.3080\n",
      "Iter 130700 | D_loss: 1.8374 | G_loss: -0.8937\n",
      "Iter 130800 | D_loss: 1.5718 | G_loss: 0.3015\n",
      "Iter 130900 | D_loss: 1.6726 | G_loss: 1.0385\n",
      "Iter 131000 | D_loss: 1.6814 | G_loss: 0.5987\n",
      "Iter 131100 | D_loss: 1.5546 | G_loss: 0.5348\n",
      "Iter 131200 | D_loss: 1.5793 | G_loss: 0.8334\n",
      "Iter 131300 | D_loss: 1.7432 | G_loss: 0.2086\n",
      "Iter 131400 | D_loss: 1.7480 | G_loss: 1.1549\n",
      "Iter 131500 | D_loss: 1.6002 | G_loss: 0.3751\n",
      "Iter 131600 | D_loss: 1.6672 | G_loss: 0.4594\n",
      "Iter 131700 | D_loss: 1.5545 | G_loss: 0.3900\n",
      "Iter 131800 | D_loss: 1.6291 | G_loss: 0.5120\n",
      "Iter 131900 | D_loss: 1.5864 | G_loss: 0.8517\n",
      "Iter 132000 | D_loss: 1.6056 | G_loss: 1.1949\n",
      "Iter 132100 | D_loss: 1.7248 | G_loss: 0.2193\n",
      "Iter 132200 | D_loss: 1.5358 | G_loss: 0.5844\n",
      "Iter 132300 | D_loss: 1.6172 | G_loss: 0.4731\n",
      "Iter 132400 | D_loss: 1.5762 | G_loss: 0.7790\n",
      "Iter 132500 | D_loss: 1.6446 | G_loss: 1.0685\n",
      "Iter 132600 | D_loss: 1.5788 | G_loss: 0.3167\n",
      "Iter 132700 | D_loss: 1.7726 | G_loss: 1.1289\n",
      "Iter 132800 | D_loss: 1.5643 | G_loss: 0.3373\n",
      "Iter 132900 | D_loss: 1.6657 | G_loss: 0.5810\n",
      "Iter 133000 | D_loss: 1.6304 | G_loss: 0.2399\n",
      "Iter 133100 | D_loss: 1.6361 | G_loss: -0.4721\n",
      "Iter 133200 | D_loss: 1.6018 | G_loss: 0.4092\n",
      "Iter 133300 | D_loss: 1.4833 | G_loss: 0.3054\n",
      "Iter 133400 | D_loss: 1.5721 | G_loss: 0.1700\n",
      "Iter 133500 | D_loss: 1.6149 | G_loss: 0.6207\n",
      "Iter 133600 | D_loss: 1.5053 | G_loss: -0.0719\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = load_config()\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
