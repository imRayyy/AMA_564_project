# AMA_564_project

**If there is any problem, please email me at [24064519g@connect.polyu.hk](mailto:24064519g@connect.polyu.hk) .**

# Project Overview
This project implements an Art Generation Adversarial Network (ArtGAN) using PyTorch, trained on the WikiArt dataset.However, since the original project(https://github.com/cs-chan/ArtGAN/tree/master) was too early and the Python and Tensorflow versions were too low, this project is based on Python 3.0 and above and pytorch. The results may deviate from the original project. Possible reasons include the differences between Tensorflow and pytorch and the version issues of related libraries.

Requirement:

- `Python`: the required packages are listed in `requirements.txt`, which is generated by `pipreqs`.  **We noticed that "pipreqs" may not be able to fully restore the dependencies between all libraries in the project environment. Since it cannot detect jupyter notebook files, this part is written manually, which may result in incomplete dependencies.**

The followings are description for each folder and file:

```
Project/
├── Dataset/wikiart/ # Contains dataset(not included due to size) and label files 
│ ├── images/ # data sets of genre in Wikiart are stored after preprocessing(not included due to size)
│ ├── labels/ # labels sets of dataset after preprocessing
│ ├── wikiart_csv/ # labels set file before preprocessing
│ ├── genre-train-index.csv # Training set labels after preprocessing
│ └── genre-val-index.csv # Validation set labels after preprocessing
│
├── genimgs/ # Generated images saved here during/after training(include generated images and real images)
├── models/ # trained model checkpoints saved here during/after training
│
├── ingest_data.py # Preprocesses raw WikiArt dataset
├── data_loader.py # Converts preprocessed data into PyTorch tensors
├── config.json # Parameters required for training
├── train.ipynb # Jupyter notebook for model training (PyTorch implementation)
├── evaluate.py # Script for model evaluation
├── genre128GAN*.py # Scripts of GAN architectures of pytorch implementation
├── requirements.txt # Dependencies
└── README.md # Project documentation (this file)
```

---

## Dataset
The **WikiArt dataset** is required but not included in this repository due to its size (25.4 GB).  
**Download link**: [WikiArt Dataset](https://drive.google.com/file/d/1vTChp3nU5GQeLkPwotrybpUGUXj12BTK/view?usp=drivesdk)  
After downloading, place the dataset in the `Dataset/wikiart` directory. 

---

## Key Components

### 1. Data Preprocessing
- **`ingest_data.py`**:  
  Preprocesses raw WikiArt images (resizing) and splits data into train/val sets.(Since the complete Wikiart dataset is too large, only the genre part is selected.)
  Usage:  
  ```bash
  python ingest_data.py --input_dir ./Dataset/WikiArt --output_dir ./Dataset/wikiart --csv_file genre

### 2. Data Loading
- **`data_loader.py`**:
 Converts preprocessed images into PyTorch tensors and creates DataLoader objects for training.

### 3. Model Training
- **`train.ipynb`**:
 Open train.ipynb in Jupyter Notebook and execute cells to start training，the required parameters are contained in **config.json** and can be read directly in jupyter notebook

 Jupyter notebook implementing the ArtGAN architecture using PyTorch. Includes:

 Generator and Discriminator networks

 Training loop with adversarial loss

 Visualization of training progress (loss curves, generated samples)

 Automatic saving of the latest model to models/

--

## Hardware Requirements
To efficiently run this project, the following computational resources are recommended:  
- **GPUs**: 1–4 NVIDIA RTX 4090D GPUs (or equivalent).  
  - The model training was validated using **1 RTX 4090D** GPU.  
- **RAM**: 32GB or higher.  
- **Storage**: SSD with at least 50GB of free space (for dataset, models, and generated images).  

For smaller-scale experiments (e.g., debugging), training on a single GPU is feasible. Multi-GPU setups can significantly accelerate training.  

**Please note that the running results may be different from the results presented in the report. We noticed that the random seed was not set in torch after the training, but we did not try again because the training process was too long and the training resources were too large.**

 
